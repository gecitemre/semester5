#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
    # corrupt all the unused registers to prevent assumptions
    irmovq $0x5710331, %rax
    irmovq $0x5710331, %rbx
    irmovq $0x5710331, %rcx
    irmovq $0x5710331, %rbp
    irmovq $0x5710331, %r8
    irmovq $0x5710331, %r9
    irmovq $0x5710331, %r10
    irmovq $0x5710331, %r11
    irmovq $0x5710331, %r12
    irmovq $0x5710331, %r13
    irmovq $0x5710331, %r14
	call abscopy		 
	halt			# should halt with abs sum in %rax
StartFun:
#/* $begin abscopy-ys */
##################################################################
# abscopy.ys - copy the absolute values of a src block of n words to dst.
# Return the sum of copied (absolute) values.
#
# name: Emre GeÃ§it
# id: 2521581

# I have tried different configurations for loop unrolling.
# Best performance is achieved with 2 loops.

# I used 3 different registers to improve the pipeline.

# I used the following three lines in order to improve performance during taking the absolute value.
#        xorq %r12, %r12         # %r12 = 0
#        subq %r10, %r12         # %r12 = -%r10
#        cmovg %r12, %r10        # if %r12 > 0, %r10 = -%r10 

# I removed and reordered some instructions in order to improve performance.

##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = n
abscopy:
##################################################################
# You can modify this portion

        irmovq $6, %r8  
        irmovq $48, %r14
        xorq %rax,%rax  
        isubq $0, %r8
        subq %r8, %rdx  
        jl Remaining    

Loop:
        mrmovq (%rdi), %r10  
        mrmovq 8(%rdi), %r11 
        mrmovq 16(%rdi), %rbp
        xorq %r12, %r12      
        xorq %r13, %r13      
        xorq %rcx, %rcx      
        subq %r10, %r12      
        cmovg %r12, %r10     
        subq %r11, %r13      
        cmovg %r13, %r11     
        subq %rbp, %rcx      
        cmovg %rcx, %rbp     
        addq %r10, %rax      
        addq %r11, %rax      
        addq %rbp, %rax      
        rmmovq %r10, (%rsi)  
        rmmovq %r11, 8(%rsi) 
        rmmovq %rbp, 16(%rsi)

        mrmovq 24(%rdi), %r10
        mrmovq 32(%rdi), %r11
        mrmovq 40(%rdi), %rbp
        xorq %r12, %r12      
        xorq %r13, %r13      
        xorq %rcx, %rcx      
        subq %r10, %r12      
        cmovg %r12, %r10     
        subq %r11, %r13      
        cmovg %r13, %r11     
        subq %rbp, %rcx      
        cmovg %rcx, %rbp     
        addq %r10, %rax      
        addq %r11, %rax      
        addq %rbp, %rax      
        rmmovq %r10, 24(%rsi)
        rmmovq %r11, 32(%rsi)
        rmmovq %rbp, 40(%rsi)
        
        addq %r14, %rsi    
        addq %r14, %rdi    
        subq %r8, %rdx     
        jge Loop           

Remaining:
        addq %r8, %rdx     
        irmovq $1, %r8     
        irmovq $8, %r14    
        je Done            
Loop2:
        mrmovq (%rdi), %r10
        xorq %r12, %r12    
        subq %r10, %r12    
        cmovg %r12, %r10   
        addq %r10, %rax    
        rmmovq %r10, (%rsi)
        addq %r14, %rsi    
        addq %r14, %rdi    
        subq %r8, %rdx     
        jg Loop2           

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
        ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end abscopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad 2
	.quad -3
	.quad 4
	.quad -5
	.quad 6
	.quad 7
	.quad -8
	.quad 9
	.quad 10
	.quad -11
	.quad -12
	.quad -13
	.quad 14
	.quad -15
	.quad 16
	.quad -17
	.quad -18
	.quad -19
	.quad -20
	.quad -21
	.quad -22
	.quad -23
	.quad 24
	.quad 25
	.quad 26
	.quad 27
	.quad 28
	.quad 29
	.quad 30
	.quad -31
	.quad 32
	.quad 33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad -38
	.quad 39
	.quad -40
	.quad -41
	.quad -42
	.quad -43
	.quad 44
	.quad -45
	.quad -46
	.quad -47
	.quad 48
	.quad 49
	.quad 50
	.quad 51
	.quad -52
	.quad -53
	.quad 54
	.quad 55
	.quad -56
	.quad -57
	.quad -58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
